{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "02e4e750-609d-4be7-bb22-c0cd8eded537",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import time\n",
    "import random\n",
    "from collections import deque\n",
    "from collections import namedtuple\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F  #activation function https://pytorch.org/docs/stable/nn.functional.html\n",
    "\n",
    "from torch.distributions.categorical import Categorical\n",
    "\n",
    "from torch.profiler import profile, record_function, ProfilerActivity\n",
    "\n",
    "import gym\n",
    "import rl_gym\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm\n",
    "import gc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dbab33c-8f3c-4d40-9a41-4e0e4b130c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "#device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "print('Using device:', device)\n",
    "#Additional Info when using cuda\n",
    "if device.type == 'cuda':\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    print('Memory Usage:')\n",
    "    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
    "    print('Cached:   ', round(torch.cuda.memory_reserved(0)/1024**3,1), 'GB')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53bcca04-bd95-441e-ac6e-37c54a4c054e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------------------------\n",
    "class SimpleMLP(nn.Module):\n",
    "\n",
    "    def __init__(self, num_observations, num_actions, num_neurons):\n",
    "        super(SimpleMLP, self).__init__()\n",
    "        \n",
    "        self.layer1 = nn.Linear(num_observations, num_neurons)\n",
    "        self.layer2 = nn.Linear(num_neurons, num_neurons)\n",
    "        self.layer3 = nn.Linear(num_neurons, num_actions)\n",
    "\n",
    "    # Called with either one element to determine next action, or a batch\n",
    "    # during optimization. Returns tensor([[left0exp,right0exp]...]).\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.layer1(x))\n",
    "        x = F.relu(self.layer2(x))\n",
    "        return self.layer3(x)\n",
    "\n",
    "#--------------------------------\n",
    "#--------------------------------\n",
    "\n",
    "def reward_to_go(rewards):\n",
    "    n = len(rewards)\n",
    "    rtgs = np.zeros_like(rewards)\n",
    "    for i in reversed(range(n)):\n",
    "        rtgs[i] = rewards[i] + (rtgs[i+1] if i+1 < n else 0)\n",
    "    return rtgs\n",
    "        \n",
    "    \n",
    "#--------------------------------\n",
    "#--------------------------------\n",
    "\n",
    "@dataclass\n",
    "class RLVPGParams:\n",
    "    num_neurons: int = 100             #number of neurons in the simple MLP used to compute Q values\n",
    "    max_episode_length: int = 600      #maximum length of an episode\n",
    "    num_epochs: int = 1000             #number of training epochs\n",
    "    num_episodes_per_epoch: int = 10   #number of episodes in each training batch\n",
    "    learning_rate: float = 0.001       #learning rate for the AdamW optimizer\n",
    "    \n",
    "class RLVPG():\n",
    "    def __init__(self, env: gym.Env, params: RLVPGParams, device):\n",
    "        \n",
    "        #get input parameters\n",
    "        self.params = params\n",
    "        self.num_observations = env.observation_space.shape[0]\n",
    "        self.num_actions = env.action_space.n\n",
    "        self.device = device\n",
    "\n",
    "        self.model = SimpleMLP(num_observations = self.num_observations, num_actions=self.num_actions, num_neurons=self.params.num_neurons).to(self.device)\n",
    "\n",
    "    def train(self, env, params):\n",
    "        \n",
    "        self.params = params\n",
    "        print(\"start training with params:\")\n",
    "        print(self.params)\n",
    "        \n",
    "        optimizer = optim.AdamW(self.model.parameters(), lr=self.params.learning_rate, amsgrad=True) #https://pytorch.org/docs/stable/generated/torch.optim.AdamW.html\n",
    "\n",
    "        scores = [] #for logging only\n",
    "                                     \n",
    "        #main training loop\n",
    "        pbar = tqdm(range(self.params.num_epochs))\n",
    "        for epoch in pbar:\n",
    "        \n",
    "            batch_obs = []\n",
    "            batch_actions = []\n",
    "            batch_weights = []\n",
    "            epoch_scores = [] #for logging only\n",
    "            \n",
    "            #run the policy for several episodes to collect a batch of trajectories\n",
    "            for episode in range(self.params.num_episodes_per_epoch):\n",
    "                \n",
    "                #initialize a new episode\n",
    "                episode_t  = 0  #time steps executed during this episode\n",
    "                rewards = []\n",
    "                obs = env.reset()\n",
    "                #obs = torch.tensor(obs, dtype=torch.float32, device=self.device).unsqueeze(0) #https://pytorch.org/docs/stable/generated/torch.unsqueeze.html\n",
    "                \n",
    "                #run one episode to completion\n",
    "                done=False\n",
    "                while not done:\n",
    "                \n",
    "                    #sample the current policy\n",
    "                    with torch.no_grad():\n",
    "                        logits = self.model(torch.tensor(obs, dtype=torch.float32, device=self.device)) #get the proba from the current policy\n",
    "                        action = Categorical(logits=logits).sample().item() #sample it\n",
    "                    \n",
    "                    #step the model with sampled action\n",
    "                    next_obs, reward, done, info = env.step(action)\n",
    "                    \n",
    "                    batch_obs.append(obs)\n",
    "                    batch_actions.append(action)\n",
    "                    rewards.append(reward)\n",
    "                    \n",
    "                    obs = next_obs\n",
    "                    episode_t += 1\n",
    "                    if episode_t>self.params.max_episode_length:\n",
    "                        done = True\n",
    "                    \n",
    "                #record what we need from this episode's trajectory\n",
    "                batch_weights += list(reward_to_go(rewards))  #cumulative rewards to end of episode for each time step\n",
    "                epoch_scores.append(sum(rewards)) #for logging only\n",
    "                \n",
    "            #adjust the policy model using the last batch of trajectories\n",
    "            optimizer.zero_grad()\n",
    "            logits = self.model(torch.tensor(np.array(batch_obs), dtype=torch.float32, device=self.device))\n",
    "            logp = Categorical(logits=logits).log_prob(torch.tensor(np.array(batch_actions), dtype=torch.float32, device=self.device))\n",
    "            loss = -(logp * torch.tensor(np.array(batch_weights), dtype=torch.float32, device=self.device)).mean()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "                \n",
    "            scores.extend(epoch_scores)\n",
    "            pbar.set_description(f'score:{np.mean(scores):.0f}')\n",
    "            pbar.refresh()\n",
    "            #print(\"epoch: %d average score:%.0f\"%(epoch, np.mean(scores)))\n",
    "\n",
    "        return scores\n",
    "    \n",
    "    def best_action(self, obs):\n",
    "        with torch.no_grad():\n",
    "            logits = self.model(torch.tensor(obs, dtype=torch.float32, device=self.device)) #get the proba from the current policy\n",
    "            action = torch.max(logits,dim=-1,keepdim=False) #return index of greatest logit, it is the most likely action\n",
    "            return action.indices.item()\n",
    "            \n",
    "        \n",
    "    def play(self, env):\n",
    "        obs = env.reset()\n",
    "        done=False\n",
    "        while not done:\n",
    "            env.render()\n",
    "            action = self.best_action(obs)\n",
    "            obs, reward, done, info = env.step(action)        \n",
    "        env.close()\n",
    "        \n",
    "    def evaluate(self, env, num_episodes=100, episode_length=600):\n",
    "        scores=[]\n",
    "        for e in range(num_episodes):\n",
    "            obs = env.reset()\n",
    "            score=0\n",
    "            done=False\n",
    "            episode_t=0\n",
    "            while not done:\n",
    "                action = self.best_action(obs)\n",
    "                obs, reward, done, info = env.step(action)\n",
    "                score += reward\n",
    "                episode_t +=1\n",
    "                if episode_t>episode_length: #force episode termination\n",
    "                    done=True\n",
    "            scores.append(score)\n",
    "        mean_reward = np.mean(scores)\n",
    "        std_reward = np.std(scores)\n",
    "        return mean_reward, std_reward, scores\n",
    "\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f599228-d649-4be3-be77-b07754a370bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#env = gym.wrappers.TimeLimit(gym.make(\"rl_gym/PuckWorld-v0\", fps=60), max_episode_steps=600)\n",
    "env = gym.make(\"rl_gym/PuckWorld-v0\", fps=60)\n",
    "\n",
    "print(device)\n",
    "params = RLVPGParams()\n",
    "vpg = RLVPG(env, params, device)\n",
    "\n",
    "start_time = time.time()\n",
    "mean_reward, std_reward, scores = vpg.evaluate(env, num_episodes=100, episode_length=600)\n",
    "print(f\"mean_reward:{mean_reward:.2f} +/- {std_reward:.2f}\")\n",
    "print(\"--- %.1f seconds ---\" % (time.time() - start_time))\n",
    "#print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccfd57eb-e7d3-4cd8-b612-2576298d9c61",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "params = RLVPGParams(num_epochs=10_000, num_episodes_per_epoch=50)\n",
    "\n",
    "print('train')\n",
    "start_time = time.time()\n",
    "scores = vpg.train(env, params)\n",
    "print(\"--- %.1f minutes ---\" % ((time.time() - start_time)/60))\n",
    "\n",
    "plt.plot(scores)\n",
    "window=20\n",
    "y = np.array(scores)\n",
    "plt.plot(np.convolve(y, np.ones(window)/window, mode='valid'))\n",
    "plt.show()\n",
    "\n",
    "print('evaluate')\n",
    "start_time = time.time()\n",
    "mean_reward, std_reward, scores = vpg.evaluate(env, num_episodes=100, episode_length=600)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "print(f\"mean_reward:{mean_reward:.2f} +/- {std_reward:.2f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "875fbebc-1a31-44cf-ba71-45126dcffcd5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ab9b44-4459-421e-8c02-6075dc56eaf1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "scenarios = [RLDQNParams(num_neurons=128, train_period=1, gamma=0.90, memory_batch=250, epsilon_half_life=100, max_time_steps=400*1000, learning_rate=0.00001),\n",
    "             RLDQNParams(num_neurons=128, train_period=1, gamma=0.90, memory_batch=250, epsilon_half_life=100, max_time_steps=400*1000, learning_rate=0.0001),\n",
    "             RLDQNParams(num_neurons=128, train_period=1, gamma=0.90, memory_batch=250, epsilon_half_life=100, max_time_steps=400*1000, learning_rate=0.001),\n",
    "             RLDQNParams(num_neurons=128, train_period=1, gamma=0.90, memory_batch=250, epsilon_half_life=400, max_time_steps=400*1000, learning_rate=0.01),\n",
    "            ]\n",
    "results = []\n",
    "\n",
    "for params in scenarios:\n",
    "    \n",
    "    dqn = RLDQN(env, params, device)\n",
    "    \n",
    "    print('=======================')\n",
    "    print('\\t\\ttrain')\n",
    "    start_time = time.time()\n",
    "    dqn.train(env, params)\n",
    "    training_time = time.time() - start_time\n",
    "    print(\"--- %s minutes ---\" % (training_time/60))\n",
    "    \n",
    "    log = TrainingRecord(*zip(*dqn.training_record))\n",
    "    #y = np.array(log.loss_mean)\n",
    "    #y = np.array(log.loss_std)\n",
    "    y = np.array(log.score)\n",
    "    #y = np.array(log.epsilon)\n",
    "    window=50\n",
    "    y_runningmean = np.convolve(y, np.ones(window)/window, mode='valid')\n",
    "    plt.plot(y)\n",
    "    plt.plot(range(int(window/2),len(y)-int(window/2)+1),y_runningmean)\n",
    "    plt.show()\n",
    "       \n",
    "\n",
    "    print('\\t\\tevaluate')\n",
    "    start_time = time.time()\n",
    "    mean_reward, std_reward, scores = dqn.evaluate(env, num_episodes=100, episode_length=600)\n",
    "    evaluating_time = time.time() - start_time\n",
    "    print(\"--- %s seconds ---\" % (evaluating_time))\n",
    "    print(f\"mean_reward:{mean_reward:.2f} +/- {std_reward:.2f}\")\n",
    "    \n",
    "    r = {\"params\": params, \n",
    "         \"log\":TrainingRecord(*zip(*dqn.training_record)),\n",
    "         \"mean_reward\": mean_reward,\n",
    "         \"std_reward\": std_reward,\n",
    "         \"training_time\": training_time,\n",
    "         \"evaluating_time\": evaluating_time   \n",
    "    }\n",
    "    results.append(r)\n",
    "\n",
    "    print('=======================')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b7c877-13a8-46df-a379-c48cd9c3ff7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "window=50\n",
    "\n",
    "print(\"Timing results\")\n",
    "for r in results:\n",
    "    label = '%f'%(r[\"params\"].learning_rate)\n",
    "    print(label, r[\"training_time\"]/60, r[\"evaluating_time\"])\n",
    "\n",
    "print(\"\\nPerformance results\")\n",
    "for r in results:\n",
    "    label = '%f'%(r[\"params\"].learning_rate)\n",
    "    print(label, r[\"mean_reward\"], r[\"std_reward\"])\n",
    "\n",
    "    y = np.array(r[\"log\"].score)\n",
    "#    y = np.array(r[\"log\"].loss_mean)\n",
    "#    y = np.array(r[\"log\"].loss_std)\n",
    "#    plt.plot(y, label=label)\n",
    "    plt.plot(np.convolve(y, np.ones(window)/window, mode='valid'), label=label)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b551e0d1-3b7c-4b24-9189-6e9ffd87f85f",
   "metadata": {},
   "outputs": [],
   "source": [
    "scenarios = [RLDQNParams(num_neurons=128, train_period=1, gamma=0.90, memory_batch=250, epsilon_half_life=50),\n",
    "             RLDQNParams(num_neurons=128, train_period=1, gamma=0.90, memory_batch=250, epsilon_half_life=100),\n",
    "             RLDQNParams(num_neurons=128, train_period=1, gamma=0.90, memory_batch=250, epsilon_half_life=400),\n",
    "            ]\n",
    "#max_time_steps: int = 1000*600\n",
    "results = []\n",
    "\n",
    "for params in scenarios:\n",
    "    \n",
    "    dqn = RLDQN(env, params, device)\n",
    "    \n",
    "    print('=======================')\n",
    "    print('\\t\\ttrain')\n",
    "    start_time = time.time()\n",
    "    dqn.train(env, params)\n",
    "    training_time = time.time() - start_time\n",
    "    print(\"--- %s minutes ---\" % (training_time/60))\n",
    "    \n",
    "    log = TrainingRecord(*zip(*dqn.training_record))\n",
    "    #y = np.array(log.loss_mean)\n",
    "    #y = np.array(log.loss_std)\n",
    "    y = np.array(log.score)\n",
    "    #y = np.array(log.epsilon)\n",
    "    window=50\n",
    "    y_runningmean = np.convolve(y, np.ones(window)/window, mode='valid')\n",
    "    plt.plot(y)\n",
    "    plt.plot(range(int(window/2),len(y)-int(window/2)+1),y_runningmean)\n",
    "    plt.show()\n",
    "       \n",
    "\n",
    "    print('\\t\\tevaluate')\n",
    "    start_time = time.time()\n",
    "    mean_reward, std_reward, scores = dqn.evaluate(env, num_episodes=100, episode_length=600)\n",
    "    evaluating_time = time.time() - start_time\n",
    "    print(\"--- %s seconds ---\" % (evaluating_time))\n",
    "    print(f\"mean_reward:{mean_reward:.2f} +/- {std_reward:.2f}\")\n",
    "    \n",
    "    r = {\"params\": params, \n",
    "         \"log\":TrainingRecord(*zip(*dqn.training_record)),\n",
    "         \"mean_reward\": mean_reward,\n",
    "         \"std_reward\": std_reward,\n",
    "         \"training_time\": training_time,\n",
    "         \"evaluating_time\": evaluating_time   \n",
    "    }\n",
    "    results.append(r)\n",
    "\n",
    "    print('=======================')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4bc25b6-e690-4a44-aa8d-0b432c7f42b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "window=50\n",
    "\n",
    "print(\"Timing results\")\n",
    "for r in results:\n",
    "    label = '%d'%(r[\"params\"].epsilon_half_life)\n",
    "    print(label, r[\"training_time\"]/60, r[\"evaluating_time\"])\n",
    "\n",
    "print(\"\\nPerformance results\")\n",
    "for r in results:\n",
    "    label = '%d'%(r[\"params\"].epsilon_half_life)\n",
    "    print(label, r[\"mean_reward\"], r[\"std_reward\"])\n",
    "\n",
    "    y = np.array(r[\"log\"].score)\n",
    "#    y = np.array(r[\"log\"].loss_mean)\n",
    "#    y = np.array(r[\"log\"].loss_std)\n",
    "#    plt.plot(y, label=label)\n",
    "    plt.plot(np.convolve(y, np.ones(window)/window, mode='valid'), label=label)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d4a116-2bc3-4427-9a6a-fff7cd7e299c",
   "metadata": {},
   "outputs": [],
   "source": [
    "scenarios = [RLDQNParams(num_neurons=128, train_period=1, gamma=0.90, memory_batch=25),\n",
    "             RLDQNParams(num_neurons=128, train_period=1, gamma=0.90, memory_batch=250),\n",
    "             RLDQNParams(num_neurons=128, train_period=1, gamma=0.90, memory_batch=1_000),\n",
    "            ]\n",
    "\n",
    "results = []\n",
    "\n",
    "for params in scenarios:\n",
    "    \n",
    "    dqn = RLDQN(env, params, device)\n",
    "    \n",
    "    print('=======================')\n",
    "    print('\\t\\ttrain')\n",
    "    start_time = time.time()\n",
    "    dqn.train(env, params)\n",
    "    training_time = time.time() - start_time\n",
    "    \n",
    "    log = TrainingRecord(*zip(*dqn.training_record))\n",
    "    #y = np.array(log.loss_mean)\n",
    "    #y = np.array(log.loss_std)\n",
    "    y = np.array(log.score)\n",
    "    #y = np.array(log.epsilon)\n",
    "    window=50\n",
    "    y_runningmean = np.convolve(y, np.ones(window)/window, mode='valid')\n",
    "    plt.plot(y)\n",
    "    plt.plot(range(int(window/2),len(y)-int(window/2)+1),y_runningmean)\n",
    "    plt.show()\n",
    "       \n",
    "\n",
    "    print('\\t\\tevaluate')\n",
    "    start_time = time.time()\n",
    "    mean_reward, std_reward, scores = dqn.evaluate(env, num_episodes=100, episode_length=600)\n",
    "    evaluating_time = time.time() - start_time\n",
    "    print(f\"mean_reward:{mean_reward:.2f} +/- {std_reward:.2f}\")\n",
    "    \n",
    "    r = {\"params\": params, \n",
    "         \"log\":TrainingRecord(*zip(*dqn.training_record)),\n",
    "         \"mean_reward\": mean_reward,\n",
    "         \"std_reward\": std_reward,\n",
    "         \"training_time\": training_time,\n",
    "         \"evaluating_time\": evaluating_time   \n",
    "    }\n",
    "    results.append(r)\n",
    "\n",
    "    print(\"--- %s minutes ---\" % ((time.time() - start_time)/60))\n",
    "    print('=======================')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43564637-b79e-4b75-af50-1e6f076be4a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "window=50\n",
    "\n",
    "print(\"Timing results\")\n",
    "for r in results:\n",
    "    label = '%d-%.2f-%d'%(r[\"params\"].num_neurons, r[\"params\"].gamma, r[\"params\"].memory_batch)\n",
    "    print(label, r[\"training_time\"], r[\"evaluating_time\"])\n",
    "\n",
    "print(\"\\nPerformance results\")\n",
    "for r in results:\n",
    "    label = '%d-%.2f-%d'%(r[\"params\"].num_neurons, r[\"params\"].gamma, r[\"params\"].memory_batch)\n",
    "    print(label, r[\"mean_reward\"], r[\"std_reward\"])\n",
    "\n",
    "    y = np.array(r[\"log\"].score)\n",
    "#    y = np.array(r[\"log\"].loss_mean)\n",
    "#    y = np.array(r[\"log\"].loss_std)\n",
    "#    plt.plot(y, label=label)\n",
    "    plt.plot(np.convolve(y, np.ones(window)/window, mode='valid'), label=label)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c72e7e28-550e-4e22-b868-f448ae9ec615",
   "metadata": {},
   "outputs": [],
   "source": [
    "window=50\n",
    "\n",
    "print(\"Timing results\")\n",
    "for r in results:\n",
    "    label = '%d-%f'%(r[\"params\"].num_neurons, r[\"params\"].gamma)\n",
    "    print(label, r[\"training_time\"], r[\"evaluating_time\"])\n",
    "\n",
    "print(\"\\nPerformance results\")\n",
    "for r in results:\n",
    "    label = '%d-%f'%(r[\"params\"].num_neurons, r[\"params\"].gamma)\n",
    "    print(label, r[\"mean_reward\"], r[\"std_reward\"])\n",
    "\n",
    "    y = np.array(r[\"log\"].score)\n",
    "#    y = np.array(r[\"log\"].loss_mean)\n",
    "#    y = np.array(r[\"log\"].loss_std)\n",
    "#    plt.plot(y, label=label)\n",
    "    plt.plot(np.convolve(y, np.ones(window)/window, mode='valid'), label=label)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da11acd8-9399-423d-bd06-2fb027a04a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "scenarios = [RLDQNParams(num_neurons=128, train_period=1, gamma=0.99),\n",
    "             RLDQNParams(num_neurons=128, train_period=1, gamma=0.95),\n",
    "             RLDQNParams(num_neurons=128, train_period=1, gamma=0.9),\n",
    "             RLDQNParams(num_neurons=128, train_period=1, gamma=0.8)]\n",
    "\n",
    "results = []\n",
    "\n",
    "for params in scenarios:\n",
    "    \n",
    "    dqn = RLDQN(env, params, device)\n",
    "    \n",
    "    print('=======================')\n",
    "    print('\\t\\ttrain')\n",
    "    start_time = time.time()\n",
    "    dqn.train(env, params)\n",
    "    training_time = time.time() - start_time\n",
    "    \n",
    "    log = TrainingRecord(*zip(*dqn.training_record))\n",
    "    #y = np.array(log.loss_mean)\n",
    "    #y = np.array(log.loss_std)\n",
    "    y = np.array(log.score)\n",
    "    #y = np.array(log.epsilon)\n",
    "    window=50\n",
    "    y_runningmean = np.convolve(y, np.ones(window)/window, mode='valid')\n",
    "    plt.plot(y)\n",
    "    plt.plot(range(int(window/2),len(y)-int(window/2)+1),y_runningmean)\n",
    "    plt.show()\n",
    "       \n",
    "\n",
    "    print('\\t\\tevaluate')\n",
    "    start_time = time.time()\n",
    "    mean_reward, std_reward, scores = dqn.evaluate(env, num_episodes=100, episode_length=600)\n",
    "    evaluating_time = time.time() - start_time\n",
    "    print(f\"mean_reward:{mean_reward:.2f} +/- {std_reward:.2f}\")\n",
    "    \n",
    "    r = {\"params\": params, \n",
    "         \"log\":TrainingRecord(*zip(*dqn.training_record)),\n",
    "         \"mean_reward\": mean_reward,\n",
    "         \"std_reward\": std_reward,\n",
    "         \"training_time\": training_time,\n",
    "         \"evaluating_time\": evaluating_time   \n",
    "    }\n",
    "    results.append(r)\n",
    "\n",
    "    print(\"--- %s minutes ---\" % ((time.time() - start_time)/60))\n",
    "    print('=======================')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d05ab0-f2f1-45bb-a21f-1385931870a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "window=50\n",
    "\n",
    "print(\"Timing results\")\n",
    "for r in results:\n",
    "    label = '%d-%f'%(r[\"params\"].num_neurons, r[\"params\"].gamma)\n",
    "    print(label, r[\"training_time\"], r[\"evaluating_time\"])\n",
    "\n",
    "print(\"\\nPerformance results\")\n",
    "for r in results[1:]:\n",
    "    label = '%d-%f'%(r[\"params\"].num_neurons, r[\"params\"].gamma)\n",
    "    print(label, r[\"mean_reward\"], r[\"std_reward\"])\n",
    "\n",
    "    y = np.array(r[\"log\"].score)\n",
    "#    y = np.array(r[\"log\"].loss_mean)\n",
    "#    plt.plot(y, label=label)\n",
    "    plt.plot(np.convolve(y, np.ones(window)/window, mode='valid'), label=label)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90f1999a-9c32-4505-87c0-9c0c8a915af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "window=50\n",
    "\n",
    "print(\"Timing results\")\n",
    "for r in results:\n",
    "    label = '%d-%f'%(r[\"params\"].num_neurons, r[\"params\"].gamma)\n",
    "    print(label, r[\"training_time\"], r[\"evaluating_time\"])\n",
    "\n",
    "print(\"\\nPerformance results\")\n",
    "for r in results:\n",
    "    label = '%d-%f'%(r[\"params\"].num_neurons, r[\"params\"].gamma)\n",
    "    print(label, r[\"mean_reward\"], r[\"std_reward\"])\n",
    "\n",
    "#    y = np.array(r[\"log\"].score)\n",
    "    y = np.array(r[\"log\"].loss_mean)\n",
    "#    plt.plot(y, label=label)\n",
    "    plt.plot(np.convolve(y, np.ones(window)/window, mode='valid'), label=label)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ccfd29e-938f-4e18-a132-556d642d965c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "scenarios = [RLDQNParams(num_neurons=32, train_period=1),\n",
    "             RLDQNParams(num_neurons=512, train_period=1),\n",
    "             RLDQNParams(num_neurons=1024, train_period=1),\n",
    "             RLDQNParams(num_neurons=2048, train_period=1)\n",
    "            ]\n",
    "\n",
    "results = []\n",
    "\n",
    "for params in scenarios:\n",
    "    \n",
    "    dqn = RLDQN(env, params, device)\n",
    "    \n",
    "    print('=======================')\n",
    "    print('\\t\\ttrain')\n",
    "    start_time = time.time()\n",
    "    dqn.train(env, params)\n",
    "    training_time = time.time() - start_time\n",
    "    \n",
    "    log = TrainingRecord(*zip(*dqn.training_record))\n",
    "    #y = np.array(log.loss_mean)\n",
    "    #y = np.array(log.loss_std)\n",
    "    y = np.array(log.score)\n",
    "    #y = np.array(log.epsilon)\n",
    "    window=50\n",
    "    y_runningmean = np.convolve(y, np.ones(window)/window, mode='valid')\n",
    "    plt.plot(y)\n",
    "    plt.plot(range(int(window/2),len(y)-int(window/2)+1),y_runningmean)\n",
    "    plt.show()\n",
    "       \n",
    "\n",
    "    print('\\t\\tevaluate')\n",
    "    start_time = time.time()\n",
    "    mean_reward, std_reward, scores = dqn.evaluate(env, num_episodes=100, episode_length=600)\n",
    "    evaluating_time = time.time() - start_time\n",
    "    print(f\"mean_reward:{mean_reward:.2f} +/- {std_reward:.2f}\")\n",
    "    \n",
    "    r = {\"params\": params, \n",
    "         \"log\":TrainingRecord(*zip(*dqn.training_record)),\n",
    "         \"mean_reward\": mean_reward,\n",
    "         \"std_reward\": std_reward,\n",
    "         \"training_time\": training_time,\n",
    "         \"evaluating_time\": evaluating_time   \n",
    "    }\n",
    "    results.append(r)\n",
    "\n",
    "    print(\"--- %s minutes ---\" % ((time.time() - start_time)/60))\n",
    "    print('=======================')\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b08d5ebe-d550-448a-b921-25e91754861b",
   "metadata": {},
   "outputs": [],
   "source": [
    "window=50\n",
    "\n",
    "print(\"Timing results\")\n",
    "for r in results:\n",
    "    label = '%d-%d'%(r[\"params\"].num_neurons, r[\"params\"].train_period)\n",
    "    print(label, r[\"training_time\"], r[\"evaluating_time\"])\n",
    "\n",
    "print(\"\\nPerformance results\")\n",
    "for r in results:\n",
    "    label = '%d-%d'%(r[\"params\"].num_neurons, r[\"params\"].train_period)\n",
    "    print(label, r[\"mean_reward\"], r[\"std_reward\"])\n",
    "\n",
    "#    y = np.array(r[\"log\"].score)\n",
    "    y = np.array(r[\"log\"].loss_mean)\n",
    "#    plt.plot(y, label=label)\n",
    "    plt.plot(np.convolve(y, np.ones(window)/window, mode='valid'), label=label)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "868e1364-af53-4ff0-86cb-b0bdfbc0b796",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_timing = np.array([[32,3033],[512,3058],[1024,3059],[2048,4523]])\n",
    "cpu_timing = np.array([[32,1532],[512,4814],[1024,12001],[2048,41592]])\n",
    "\n",
    "plt.plot(gpu_timing[:,0],gpu_timing[:,1], label=\"gpu\")\n",
    "plt.plot(gpu_timing[:,0],cpu_timing[:,1], label=\"cpu\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "gpu_score = np.array([[32,-179,69],[512,-159,138],[1024,-136,51],[2048,-138,75]])\n",
    "cpu_score = np.array([[32,-196,105],[512,-137,59],[1024,-138,68],[2048,-162,103]])\n",
    "\n",
    "plt.plot(gpu_score[:,0],gpu_score[:,1], label=\"gpu\")\n",
    "plt.plot(cpu_score[:,0],cpu_score[:,1], label=\"cpu\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c25d6957-2a8a-4b3e-a174-4e005771b55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "window=50\n",
    "\n",
    "print(\"Timing results\")\n",
    "for r in results:\n",
    "    label = '%d-%d'%(r[\"params\"].num_neurons, r[\"params\"].train_period)\n",
    "    print(label, r[\"training_time\"], r[\"evaluating_time\"])\n",
    "\n",
    "print(\"\\nPerformance results\")\n",
    "for r in results:\n",
    "    label = '%d-%d'%(r[\"params\"].num_neurons, r[\"params\"].train_period)\n",
    "    print(label, r[\"mean_reward\"], r[\"std_reward\"])\n",
    "\n",
    "#    y = np.array(r[\"log\"].score)\n",
    "    y = np.array(r[\"log\"].loss_mean)\n",
    "#    plt.plot(y, label=label)\n",
    "    plt.plot(np.convolve(y, np.ones(window)/window, mode='valid'), label=label)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ea8b03-942c-40d0-9725-641816a0d74b",
   "metadata": {},
   "outputs": [],
   "source": [
    "window=50\n",
    "\n",
    "print(\"Timing results\")\n",
    "for r in results:\n",
    "    label = '%d-%d'%(r[\"params\"].num_neurons, r[\"params\"].train_period)\n",
    "    print(label, r[\"training_time\"], r[\"evaluating_time\"])\n",
    "\n",
    "print(\"\\nPerformance results\")\n",
    "for r in results:\n",
    "    label = '%d-%d'%(r[\"params\"].num_neurons, r[\"params\"].train_period)\n",
    "    print(label, r[\"mean_reward\"], r[\"std_reward\"])\n",
    "\n",
    "    y = np.array(r[\"log\"].score)\n",
    "#    y = np.array(r[\"log\"].loss_mean)\n",
    "#    plt.plot(y, label=label)\n",
    "    plt.plot(np.convolve(y, np.ones(window)/window, mode='valid'), label=label)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4796afe2-20fc-442f-8190-0451ef52fe8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "window=50\n",
    "for r in results:\n",
    "    \n",
    "    label = r[\"params\"].num_neurons\n",
    "    \n",
    "    print(r[\"params\"].num_neurons, r[\"mean_reward\"], r[\"std_reward\"])\n",
    "\n",
    "    y = np.array(r[\"log\"].score)\n",
    "#    y = np.array(r[\"log\"].loss_mean)\n",
    "#    plt.plot(y, label=label)\n",
    "    plt.plot(np.convolve(y, np.ones(window)/window, mode='valid'), label=label)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db22ce1e-4682-4fd5-8d5b-6c49d08d3df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dqn_c = RLDQN(env, params, torch.device('cpu'))\n",
    "\n",
    "print(\"evaluating on GPU\")\n",
    "start_time = time.time()\n",
    "dqn.evaluate(env,num_episodes=10, episode_length=600)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "print(\"evaluating on CPU\")\n",
    "start_time = time.time()\n",
    "dqn_c.evaluate(env,num_episodes=10, episode_length=600)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0596bfab-1f80-4579-9aa6-65a339db4248",
   "metadata": {},
   "outputs": [],
   "source": [
    "with profile(activities=[ProfilerActivity.CUDA,ProfilerActivity.CPU], \n",
    "             #profile_memory=True,\n",
    "             record_shapes=True) as prof:\n",
    "    with record_function(\"eval_gpu\"):\n",
    "        dqn.evaluate(env,num_episodes=1, episode_length=600)\n",
    "\n",
    "print(prof.key_averages().table(sort_by=\"cpu_time_total\", row_limit=10))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18083b61-0fad-4879-b56b-a638f76c650f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with profile(activities=[ProfilerActivity.CUDA,ProfilerActivity.CPU], \n",
    "             #profile_memory=True,\n",
    "             record_shapes=True) as prof:\n",
    "    with record_function(\"eval_cpu\"):\n",
    "        dqn_c.evaluate(env,num_episodes=1, episode_length=600)\n",
    "        \n",
    "print(prof.key_averages().table(sort_by=\"cpu_time_total\", row_limit=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b590d1-1a35-42cf-aa99-11e2d09032f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "vpg.play(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "193f7b5c-82b7-467b-8966-fe50006a7385",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfgpu",
   "language": "python",
   "name": "tfgpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
