{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "02e4e750-609d-4be7-bb22-c0cd8eded537",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'rl_gym'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 19\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprofiler\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m profile, record_function, ProfilerActivity\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgym\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mrl_gym\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m     23\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'rl_gym'"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import time\n",
    "import random\n",
    "from collections import deque\n",
    "from collections import namedtuple\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F  #activation function https://pytorch.org/docs/stable/nn.functional.html\n",
    "from torch.profiler import profile, record_function, ProfilerActivity\n",
    "\n",
    "import gym\n",
    "import rl_gym\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(\"cpu\")  #CUDA is 3 times slower than CPU for this small model..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dbab33c-8f3c-4d40-9a41-4e0e4b130c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "#device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "print('Using device:', device)\n",
    "#Additional Info when using cuda\n",
    "if device.type == 'cuda':\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    print('Memory Usage:')\n",
    "    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
    "    print('Cached:   ', round(torch.cuda.memory_reserved(0)/1024**3,1), 'GB')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53bcca04-bd95-441e-ac6e-37c54a4c054e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------------------------\n",
    "class SimpleMLP(nn.Module):\n",
    "\n",
    "    def __init__(self, num_observations, num_actions, num_neurons):\n",
    "        super(SimpleMLP, self).__init__()\n",
    "        \n",
    "        self.layer1 = nn.Linear(num_observations, num_neurons)\n",
    "        self.layer2 = nn.Linear(num_neurons, num_neurons)\n",
    "        self.layer3 = nn.Linear(num_neurons, num_actions)\n",
    "\n",
    "    # Called with either one element to determine next action, or a batch\n",
    "    # during optimization. Returns tensor([[left0exp,right0exp]...]).\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.layer1(x))\n",
    "        x = F.relu(self.layer2(x))\n",
    "        return self.layer3(x)\n",
    "\n",
    "#--------------------------------\n",
    "#--------------------------------\n",
    "\n",
    "Transition = namedtuple('Transition', ('obs', 'action', 'reward', 'done', 'next_obs'))\n",
    "\n",
    "class ReplayMemory():\n",
    "    \n",
    "    def __init__(self, memory_size):\n",
    "        self.memory = deque([], maxlen=memory_size)\n",
    "    \n",
    "    def append(self, transition: Transition):\n",
    "        self.memory.append(transition)\n",
    "        \n",
    "    def sample(self, batch_size):\n",
    "        transitions = random.sample(self.memory, batch_size)\n",
    "        return Transition(*zip(*transitions))        #see https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html for the *zip* trick\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "    \n",
    "#--------------------------------\n",
    "#--------------------------------\n",
    "\n",
    "TrainingRecord = namedtuple('Training', \n",
    "                            ('t', 'episode', 'episode_t', 'epsilon', 'score', 'loss_mean', 'loss_std'))\n",
    "\n",
    "@dataclass\n",
    "class RLDQNParams:\n",
    "    num_neurons: int = 32              #number of neurons in the simple MLP used to compute Q values\n",
    "    max_episode_length: int = 600      #maximum length of an episode\n",
    "    max_time_steps: int = 1000*600     #number of time steps used for training the model\n",
    "    train_period: int = 1              #train the policy network every x timesteps\n",
    "    learning_rate: float = 0.0001      #learning rate for the AdamW optimizer\n",
    "    memory_size: int = 50_000          #size of replay memory, older samples are discarded\n",
    "    memory_batch: int = 64             #size of batch sampled from replay memory for each training step\n",
    "    gamma: float = 0.9                 #discount factor\n",
    "    epsilon: float = 0.5               #initial epsilon value for the epsilon-greedy policy\n",
    "    epsilon_min: float = 0.05          #minimum epsilon value for the epsilon-greedy strategy\n",
    "    epsilon_half_life: int = 200       #decrease epsilon at every time step with the given half life\n",
    "    target_update_rate: float = 0.05   #rate at which the target model is updated from the policy model\n",
    "    log_recent_episodes: int = 100     #print a message every x episodes during training loop\n",
    "    \n",
    "class RLDQN():\n",
    "    def __init__(self, env: gym.Env, params: RLDQNParams, device):\n",
    "        \n",
    "        #get input parameters\n",
    "        self.params = params\n",
    "        self.num_observations = env.observation_space.shape[0]\n",
    "        self.num_actions = env.action_space.n\n",
    "        self.device = device\n",
    "\n",
    "        self.model = SimpleMLP(num_observations = self.num_observations, num_actions=self.num_actions, num_neurons=self.params.num_neurons).to(self.device)\n",
    "        self.target_model = SimpleMLP(num_observations = self.num_observations, num_actions=self.num_actions, num_neurons=self.params.num_neurons).to(self.device)\n",
    "        self.target_model.load_state_dict(self.model.state_dict())\n",
    "\n",
    "    def play(self, env):\n",
    "        obs = env.reset()\n",
    "        done=False\n",
    "        while not done:\n",
    "            env.render()\n",
    "            obs = torch.tensor(obs, dtype=torch.float32, device=self.device).unsqueeze(0) \n",
    "            action = self.select_action(env, obs, epsilon=0.)\n",
    "            obs, reward, done, info = env.step(action.item())        \n",
    "        env.close()\n",
    "        \n",
    "    def evaluate(self, env, num_episodes=100, episode_length=600):\n",
    "        scores=[]\n",
    "        for e in range(num_episodes):\n",
    "            obs = env.reset()\n",
    "            score=0\n",
    "            done=False\n",
    "            episode_t=0\n",
    "            while not done:\n",
    "                obs = torch.tensor(obs, dtype=torch.float32, device=self.device).unsqueeze(0) #https://pytorch.org/docs/stable/generated/torch.unsqueeze.html\n",
    "                action = self.select_action(env, obs, epsilon=0.)\n",
    "                obs, reward, done, info = env.step(action.item())\n",
    "                score += reward\n",
    "                episode_t +=1\n",
    "                if episode_t>episode_length: #force episode termination\n",
    "                    done=True\n",
    "            scores.append(score)\n",
    "        mean_reward = np.mean(scores)\n",
    "        std_reward = np.std(scores)\n",
    "        return mean_reward, std_reward, scores\n",
    "\n",
    "    def train(self, env, params):\n",
    "        \n",
    "        self.params = params\n",
    "        print(\"start training with params:\")\n",
    "        print(params)\n",
    "        \n",
    "        finished_training = False\n",
    "        epsilon = self.params.epsilon\n",
    "        optimizer = optim.AdamW(self.model.parameters(), lr=self.params.learning_rate, amsgrad=True) #https://pytorch.org/docs/stable/generated/torch.optim.AdamW.html\n",
    "        criterion = nn.SmoothL1Loss() #https://pytorch.org/docs/stable/generated/torch.nn.SmoothL1Loss.html\n",
    "\n",
    "        self.memory = ReplayMemory(self.params.memory_size)\n",
    "        self.training_record = [] #array of TrainingRecord namedtuples to record progress during the training loop; one entry per episode\n",
    "        \n",
    "        obs = env.reset()\n",
    "        obs = torch.tensor(obs, dtype=torch.float32, device=self.device).unsqueeze(0) #https://pytorch.org/docs/stable/generated/torch.unsqueeze.html\n",
    "        \n",
    "        t = 0 #to measure time since beginning of training\n",
    "        episode = 0 #to measure number of episodes\n",
    "        episode_t = 0 #to measure length of current episode\n",
    "        score = 0. #to track score of current episode\n",
    "        loss_record = [] #track loss during episode\n",
    "\n",
    "        #training loop\n",
    "        #environment is reset within the loop whenever it reaches a terminal step or when it reaches the max lenghth of an episode\n",
    "        #tqdm_bar = tqdm(range(self.params.max_time_steps))\n",
    "        while not finished_training:\n",
    "           \n",
    "            #generate the next transition and add it to replay memory\n",
    "\n",
    "            action = self.select_action(env, obs, epsilon)\n",
    "            next_obs, reward, done, info = env.step(action.item())\n",
    "\n",
    "            score += reward #episode's score\n",
    "            #obs is already a tensor on device\n",
    "            #action is already a tensor on device, see select_action()\n",
    "            next_obs = torch.tensor(next_obs, dtype=torch.float32, device=self.device).unsqueeze(0) #https://pytorch.org/docs/stable/generated/torch.unsqueeze.html\n",
    "            reward = torch.tensor([reward], dtype=torch.float32, device=self.device)\n",
    "            not_done = torch.tensor([0. if done else 1.], dtype=torch.float32, device=self.device)  #flip done to more easily calculate td_targets during training (see below)\n",
    "            self.memory.append(Transition(obs, action, reward, not_done, next_obs))\n",
    "\n",
    "            #train the model\n",
    "\n",
    "            if len(self.memory) > self.params.memory_batch and t % self.params.train_period == 0:\n",
    "\n",
    "                batch = self.memory.sample(self.params.memory_batch)\n",
    "\n",
    "                #calculate the current model's estimate of the Q value corresponding to the observation and action taken at the time\n",
    "                #Q = model[obs][action]\n",
    "                q = self.model(torch.cat(batch.obs)).gather(1, torch.cat(batch.action)).squeeze(1) \n",
    "\n",
    "                #calculate the temporal difference's target value, which is the reward + the discounted Q value of the following state\n",
    "                #use the 'target' model for stability, the target model evolves more slowly than the policy model\n",
    "                #non_terminal is 0 if the transition was terminal, 1 otherwise (this is the flip of the 'done' value returned by the env\n",
    "                #TD Target = reward + non_terminal * gamma * max_action[model(next_obs)]\n",
    "                with torch.no_grad():\n",
    "                    td_targets = self.target_model(torch.cat(batch.next_obs)) #calculate the Q values on the resulting state of the transition\n",
    "                    td_targets = td_targets.max(1).values #get the max Q value across possible actions\n",
    "                    td_targets *= torch.cat(batch.done) #keep target Q value estimate only for non-terminal transition .done is 0 is we reached the terminal state at this transition, 1 otherwise (see how the done flag is inverted when recording it in replay memory\n",
    "                    td_targets *= self.params.gamma #discount\n",
    "                    td_targets += torch.cat(batch.reward) #add reward for both terminal and non-terminal transitions\n",
    "\n",
    "                #calculate the temporal difference loss\n",
    "                #the criterion function returns the average loss over the transitions sampled in this batch\n",
    "                #the loss function is the square of the error is the error is less than one, or the abs value of the error otherwise\n",
    "                #this is more robust to outliers than pure squared error\n",
    "                #td_loss = mean_batch[(td_targets-q)^2]\n",
    "                td_loss = criterion(td_targets, q)\n",
    "                loss_record.append(td_loss.item())\n",
    "\n",
    "                #update the model's parameters to minimize td loss by stochastic gradient descent\n",
    "                optimizer.zero_grad()\n",
    "                td_loss.backward()\n",
    "                torch.nn.utils.clip_grad_value_(self.model.parameters(),100) #https://pytorch.org/docs/stable/generated/torch.nn.utils.clip_grad_value_.html\n",
    "                optimizer.step()\n",
    "\n",
    "                #update the parameters theta' of the target model from the policy model\n",
    "                #theta' <- update_rate*theta + (1-update_rate)*theta'\n",
    "                with torch.no_grad():\n",
    "                    target_params = self.target_model.state_dict() #https://pytorch.org/tutorials/recipes/recipes/what_is_state_dict.html\n",
    "                    policy_params = self.model.state_dict()\n",
    "                    for key in policy_params:\n",
    "                        target_params[key] = policy_params[key]*self.params.target_update_rate + target_params[key]*(1. - self.params.target_update_rate)\n",
    "                    self.target_model.load_state_dict(target_params)\n",
    "\n",
    "\n",
    "            episode_t += 1\n",
    "            if done or episode_t>self.params.max_episode_length: \n",
    "\n",
    "                #end of episode, prepare next one\n",
    "\n",
    "                if epsilon > self.params.epsilon_min:\n",
    "                    epsilon -= epsilon * math.log(2)/self.params.epsilon_half_life\n",
    "                \n",
    "                #log temporary results\n",
    "                loss_mean = np.mean(loss_record)\n",
    "                loss_std = np.std(loss_record)\n",
    "                loss_record=[]\n",
    "                r = TrainingRecord(t=t, episode=episode, episode_t=episode_t, epsilon=epsilon, score=score, loss_mean=loss_mean, loss_std=loss_std)\n",
    "                self.training_record.append(r)\n",
    "                \n",
    "                if episode % self.params.log_recent_episodes==0:\n",
    "                    print(r)\n",
    "\n",
    "                #start a new episode\n",
    "                #tqdm_bar.set_postfix({'episode': episode})\n",
    "                episode += 1\n",
    "                episode_t = 0\n",
    "                score = 0.\n",
    "                next_obs = env.reset()\n",
    "                next_obs = torch.tensor(next_obs, dtype=torch.float32, device=device).unsqueeze(0) \n",
    "\n",
    "            #prepare next iteration\n",
    "            \n",
    "            obs = next_obs\n",
    "\n",
    "            t += 1\n",
    "            if t>self.params.max_time_steps:\n",
    "                finished_training=True  #will exit the main training loop\n",
    "\n",
    "        \n",
    "\n",
    "    def select_action(self, env, obs, epsilon):\n",
    "        if random.random()<epsilon:\n",
    "            action = env.action_space.sample()\n",
    "            return torch.tensor([[action]], device=self.device, dtype=torch.long)\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                q_values = self.model(obs)\n",
    "                max_q = q_values.max(1).indices\n",
    "                return max_q.view(1,1)\n",
    "\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f599228-d649-4be3-be77-b07754a370bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#env = gym.wrappers.TimeLimit(gym.make(\"rl_gym/PuckWorld-v0\", fps=60), max_episode_steps=600)\n",
    "env = gym.make(\"rl_gym/PuckWorld-v0\", fps=60)\n",
    "\n",
    "params = RLDQNParams()\n",
    "'''class RLDQNParams:\n",
    "    num_neurons: int = 32              #number of neurons in the simple MLP used to compute Q values\n",
    "    max_episode_length: int = 600      #maximum length of an episode\n",
    "    max_time_steps: int = 1000*600     #number of time steps used for training the model\n",
    "    train_period: int = 10             #train the policy network every x timesteps\n",
    "    learning_rate: float = 0.0001      #learning rate for the AdamW optimizer\n",
    "    memory_size: int = 50_000          #size of replay memory, older samples are discarded\n",
    "    memory_batch: int = 64             #size of batch sampled from replay memory for each training step\n",
    "    gamma: float = 0.9                 #discount factor\n",
    "    epsilon: float = 0.5               #initial epsilon value for the epsilon-greedy policy\n",
    "    epsilon_min: float = 0.05          #minimum epsilon value for the epsilon-greedy strategy\n",
    "    epsilon_half_life: int = 200       #decrease epsilon at every time step with the given half life\n",
    "    target_update_rate: float = 0.05   #rate at which the target model is updated from the policy model\n",
    "    log_recent_episodes: int = 100     #print a message every x episodes during training loop\n",
    "'''\n",
    "\n",
    "#params.num_neurons = 64\n",
    "#params.train_period: int = 1              #train the policy network every x timesteps\n",
    "#params.max_time_steps: int = 500*600      #number of time steps used for training the model\n",
    "\n",
    "print(device)\n",
    "\n",
    "dqn = RLDQN(env, params, device)\n",
    "\n",
    "start_time = time.time()\n",
    "mean_reward, std_reward, scores = dqn.evaluate(env, num_episodes=100, episode_length=600)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "print(f\"mean_reward:{mean_reward:.2f} +/- {std_reward:.2f}\")\n",
    "#print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccfd57eb-e7d3-4cd8-b612-2576298d9c61",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "print('train')\n",
    "start_time = time.time()\n",
    "dqn.train(env, params)\n",
    "print(\"--- %s minutes ---\" % ((time.time() - start_time)/60))\n",
    "\n",
    "print('evaluate')\n",
    "start_time = time.time()\n",
    "mean_reward, std_reward, scores = dqn.evaluate(env, num_episodes=100, episode_length=600)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "print(f\"mean_reward:{mean_reward:.2f} +/- {std_reward:.2f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "875fbebc-1a31-44cf-ba71-45126dcffcd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "log = TrainingRecord(*zip(*dqn.training_record))\n",
    "\n",
    "#y = np.array(log.loss_mean)\n",
    "#y = np.array(log.loss_std)\n",
    "y = np.array(log.score)\n",
    "#y = np.array(log.epsilon)\n",
    "\n",
    "window=50\n",
    "y_runningmean = np.convolve(y, np.ones(window)/window, mode='valid')\n",
    "plt.plot(y)\n",
    "plt.plot(range(int(window/2),len(y)-int(window/2)+1),y_runningmean)\n",
    "\n",
    "y_runningmean.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ccfd29e-938f-4e18-a132-556d642d965c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "scenarios = [RLDQNParams(num_neurons=32, train_period=1),\n",
    "             RLDQNParams(num_neurons=512, train_period=1),\n",
    "             RLDQNParams(num_neurons=1024, train_period=1),\n",
    "             RLDQNParams(num_neurons=2048, train_period=1),\n",
    "            ]\n",
    "\n",
    "results = []\n",
    "\n",
    "for params in scenarios:\n",
    "    \n",
    "    dqn = RLDQN(env, params, device)\n",
    "    \n",
    "    print('=======================')\n",
    "    print('\\t\\ttrain')\n",
    "    start_time = time.time()\n",
    "    dqn.train(env, params)\n",
    "    training_time = time.time() - start_time\n",
    "    \n",
    "    log = TrainingRecord(*zip(*dqn.training_record))\n",
    "    #y = np.array(log.loss_mean)\n",
    "    #y = np.array(log.loss_std)\n",
    "    y = np.array(log.score)\n",
    "    #y = np.array(log.epsilon)\n",
    "    window=50\n",
    "    y_runningmean = np.convolve(y, np.ones(window)/window, mode='valid')\n",
    "    plt.plot(y)\n",
    "    plt.plot(range(int(window/2),len(y)-int(window/2)+1),y_runningmean)\n",
    "    plt.show()\n",
    "       \n",
    "\n",
    "    print('\\t\\tevaluate')\n",
    "    start_time = time.time()\n",
    "    mean_reward, std_reward, scores = dqn.evaluate(env, num_episodes=100, episode_length=600)\n",
    "    evaluating_time = time.time() - start_time\n",
    "    print(f\"mean_reward:{mean_reward:.2f} +/- {std_reward:.2f}\")\n",
    "    \n",
    "    r = {\"params\": params, \n",
    "         \"log\":TrainingRecord(*zip(*dqn.training_record)),\n",
    "         \"mean_reward\": mean_reward,\n",
    "         \"std_reward\": std_reward,\n",
    "         \"training_time\": training_time,\n",
    "         \"evaluating_time\": evaluating_time   \n",
    "    }\n",
    "    results.append(r)\n",
    "\n",
    "    print(\"--- %s minutes ---\" % ((time.time() - start_time)/60))\n",
    "    print('=======================')\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c25d6957-2a8a-4b3e-a174-4e005771b55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "window=50\n",
    "\n",
    "print(\"Timing results\")\n",
    "for r in results:\n",
    "    label = '%d-%d'%(r[\"params\"].num_neurons, r[\"params\"].train_period)\n",
    "    print(label, r[\"training_time\"], r[\"evaluating_time\"])\n",
    "\n",
    "print(\"\\nPerformance results\")\n",
    "for r in results:\n",
    "    label = '%d-%d'%(r[\"params\"].num_neurons, r[\"params\"].train_period)\n",
    "    print(label, r[\"mean_reward\"], r[\"std_reward\"])\n",
    "\n",
    "#    y = np.array(r[\"log\"].score)\n",
    "    y = np.array(r[\"log\"].loss_mean)\n",
    "#    plt.plot(y, label=label)\n",
    "    plt.plot(np.convolve(y, np.ones(window)/window, mode='valid'), label=label)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ea8b03-942c-40d0-9725-641816a0d74b",
   "metadata": {},
   "outputs": [],
   "source": [
    "window=50\n",
    "\n",
    "print(\"Timing results\")\n",
    "for r in results:\n",
    "    label = '%d-%d'%(r[\"params\"].num_neurons, r[\"params\"].train_period)\n",
    "    print(label, r[\"training_time\"], r[\"evaluating_time\"])\n",
    "\n",
    "print(\"\\nPerformance results\")\n",
    "for r in results:\n",
    "    label = '%d-%d'%(r[\"params\"].num_neurons, r[\"params\"].train_period)\n",
    "    print(label, r[\"mean_reward\"], r[\"std_reward\"])\n",
    "\n",
    "    y = np.array(r[\"log\"].score)\n",
    "#    y = np.array(r[\"log\"].loss_mean)\n",
    "#    plt.plot(y, label=label)\n",
    "    plt.plot(np.convolve(y, np.ones(window)/window, mode='valid'), label=label)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4796afe2-20fc-442f-8190-0451ef52fe8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "window=50\n",
    "for r in results:\n",
    "    \n",
    "    label = r[\"params\"].num_neurons\n",
    "    \n",
    "    print(r[\"params\"].num_neurons, r[\"mean_reward\"], r[\"std_reward\"])\n",
    "\n",
    "    y = np.array(r[\"log\"].score)\n",
    "#    y = np.array(r[\"log\"].loss_mean)\n",
    "#    plt.plot(y, label=label)\n",
    "    plt.plot(np.convolve(y, np.ones(window)/window, mode='valid'), label=label)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db22ce1e-4682-4fd5-8d5b-6c49d08d3df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dqn_c = RLDQN(env, params, torch.device('cpu'))\n",
    "\n",
    "print(\"evaluating on GPU\")\n",
    "start_time = time.time()\n",
    "dqn.evaluate(env,num_episodes=10, episode_length=600)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "print(\"evaluating on CPU\")\n",
    "start_time = time.time()\n",
    "dqn_c.evaluate(env,num_episodes=10, episode_length=600)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0596bfab-1f80-4579-9aa6-65a339db4248",
   "metadata": {},
   "outputs": [],
   "source": [
    "with profile(activities=[ProfilerActivity.CUDA,ProfilerActivity.CPU], \n",
    "             #profile_memory=True,\n",
    "             record_shapes=True) as prof:\n",
    "    with record_function(\"eval_gpu\"):\n",
    "        dqn.evaluate(env,num_episodes=1, episode_length=600)\n",
    "\n",
    "print(prof.key_averages().table(sort_by=\"cpu_time_total\", row_limit=10))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18083b61-0fad-4879-b56b-a638f76c650f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with profile(activities=[ProfilerActivity.CUDA,ProfilerActivity.CPU], \n",
    "             #profile_memory=True,\n",
    "             record_shapes=True) as prof:\n",
    "    with record_function(\"eval_cpu\"):\n",
    "        dqn_c.evaluate(env,num_episodes=1, episode_length=600)\n",
    "        \n",
    "print(prof.key_averages().table(sort_by=\"cpu_time_total\", row_limit=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b590d1-1a35-42cf-aa99-11e2d09032f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn.play(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "193f7b5c-82b7-467b-8966-fe50006a7385",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfgpu",
   "language": "python",
   "name": "tfgpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
